## CrawlerðŸ”Ž
<pre> 
Dependencies
1.cat 
2.find 
3.python3 
4.re //regular expression 
5.tested on linux system (archx64)

Lookup methods
1.used regex / regular expression for string lookup in the files
2.cat is used to pipeline the data to stdin into test2.py
3.find is used for directory travesal and adding the file in history.txt 

Steps
1.run the run.py  file and in the directory on the command line 
it will create a history.txt  with all the file present 
history.txt isn't stored its removed at the end of process 

Todo
1.create storage or display the highest number of occurances 
2.fix the unicode decode error // prevent it from accessing the non-decodable format 
3.use json for results and storage
4.add a option for storing history.txt
</pre> 
![screenshot][https://github.com/UmangX/crawler/blob/master/screenshot.png?raw=true]
